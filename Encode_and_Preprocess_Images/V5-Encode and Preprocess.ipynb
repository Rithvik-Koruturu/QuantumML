{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d59d4955-5bcb-46ad-a73a-e337cac083d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import pennylane as qml\n",
    "from pennylane import numpy as pnp\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4728b604-3bf6-498e-aaf6-8a4af27d9b2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 2000 images with shape (64, 64)\n"
     ]
    }
   ],
   "source": [
    "dataset_path = \"D:/datasets/PetImages\"\n",
    "categories = [\"Cat\", \"Dog\"]\n",
    "num_images_per_category = 1000\n",
    "images = []\n",
    "labels = []\n",
    "\n",
    "for label, category in enumerate(categories):\n",
    "    category_path = os.path.join(dataset_path, category)\n",
    "    count = 0\n",
    "    for filename in os.listdir(category_path):\n",
    "        if count >= num_images_per_category:\n",
    "            break\n",
    "        try:\n",
    "            img_path = os.path.join(category_path, filename)\n",
    "            img = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE)\n",
    "            if img is None:\n",
    "                continue\n",
    "            img = cv2.resize(img, (64, 64))  # Resize to standard size\n",
    "            images.append(img)\n",
    "            labels.append(label)\n",
    "            count += 1\n",
    "        except:\n",
    "            continue  # Ignore unreadable files\n",
    "images = np.array(images)\n",
    "labels = np.array(labels)\n",
    "print(f\"Loaded {len(images)} images with shape {images[0].shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2b13f112-01a2-4648-a5ff-d227e14bf85f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_hybrid_features(img, pca_components=32):\n",
    "    # --- Classical Features: ORB ---\n",
    "    orb = cv2.ORB_create(nfeatures=64)\n",
    "    keypoints, descriptors = orb.detectAndCompute(img, None)\n",
    "    if descriptors is None:\n",
    "        descriptors = np.zeros((64, 32), dtype=np.uint8)\n",
    "    elif descriptors.shape[0] < 64:\n",
    "        descriptors = np.vstack([descriptors, np.zeros((64 - descriptors.shape[0], 32), dtype=np.uint8)])\n",
    "    else:\n",
    "        descriptors = descriptors[:64]\n",
    "    orb_features = descriptors.flatten()  # Shape: 64 x 32 = 2048\n",
    "\n",
    "    # --- Quantum-aware: Flatten image for amplitude embedding ---\n",
    "    img_norm = img / 255.0\n",
    "    flat = img_norm.flatten()\n",
    "    if len(flat) < 256:\n",
    "        padded = np.pad(flat, (0, 256 - len(flat)))\n",
    "    else:\n",
    "        padded = flat[:256]\n",
    "    amp_features = padded  # For amplitude encoding (real-valued)\n",
    "\n",
    "    # --- PCA Projection for QAOA Encoding Sim ---\n",
    "    pca = PCA(n_components=pca_components)\n",
    "    flattened_img = img.flatten().reshape(1, -1)  # shape (1, 4096) or so\n",
    "    pca_data = pca.fit_transform(flattened_img).flatten()\n",
    "\n",
    "\n",
    "    # --- Combine all ---\n",
    "    hybrid_feature_vector = np.concatenate((orb_features, amp_features, pca_data))\n",
    "    return hybrid_feature_vector\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7d5ecf66-7f98-4028-8d46-af059ba1f84c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting hybrid features:   0%|                                                             | 0/2000 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "n_components=32 must be between 0 and min(n_samples, n_features)=1 with svd_solver='full'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m hybrid_features \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m img \u001b[38;5;129;01min\u001b[39;00m tqdm(images, desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExtracting hybrid features\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m----> 3\u001b[0m     feats \u001b[38;5;241m=\u001b[39m \u001b[43mextract_hybrid_features\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      4\u001b[0m     hybrid_features\u001b[38;5;241m.\u001b[39mappend(feats)\n\u001b[0;32m      6\u001b[0m hybrid_features \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(hybrid_features)\n",
      "Cell \u001b[1;32mIn[9], line 25\u001b[0m, in \u001b[0;36mextract_hybrid_features\u001b[1;34m(img, pca_components)\u001b[0m\n\u001b[0;32m     23\u001b[0m pca \u001b[38;5;241m=\u001b[39m PCA(n_components\u001b[38;5;241m=\u001b[39mpca_components)\n\u001b[0;32m     24\u001b[0m flattened_img \u001b[38;5;241m=\u001b[39m img\u001b[38;5;241m.\u001b[39mflatten()\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)  \u001b[38;5;66;03m# shape (1, 4096) or so\u001b[39;00m\n\u001b[1;32m---> 25\u001b[0m pca_data \u001b[38;5;241m=\u001b[39m \u001b[43mpca\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mflattened_img\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mflatten()\n\u001b[0;32m     28\u001b[0m \u001b[38;5;66;03m# --- Combine all ---\u001b[39;00m\n\u001b[0;32m     29\u001b[0m hybrid_feature_vector \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mconcatenate((orb_features, amp_features, pca_data))\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\utils\\_set_output.py:316\u001b[0m, in \u001b[0;36m_wrap_method_output.<locals>.wrapped\u001b[1;34m(self, X, *args, **kwargs)\u001b[0m\n\u001b[0;32m    314\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(f)\n\u001b[0;32m    315\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapped\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m--> 316\u001b[0m     data_to_wrap \u001b[38;5;241m=\u001b[39m f(\u001b[38;5;28mself\u001b[39m, X, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    317\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data_to_wrap, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[0;32m    318\u001b[0m         \u001b[38;5;66;03m# only wrap the first output for cross decomposition\u001b[39;00m\n\u001b[0;32m    319\u001b[0m         return_tuple \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    320\u001b[0m             _wrap_data_with_container(method, data_to_wrap[\u001b[38;5;241m0\u001b[39m], X, \u001b[38;5;28mself\u001b[39m),\n\u001b[0;32m    321\u001b[0m             \u001b[38;5;241m*\u001b[39mdata_to_wrap[\u001b[38;5;241m1\u001b[39m:],\n\u001b[0;32m    322\u001b[0m         )\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\base.py:1473\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1466\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1468\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1469\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1470\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1471\u001b[0m     )\n\u001b[0;32m   1472\u001b[0m ):\n\u001b[1;32m-> 1473\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fit_method(estimator, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\decomposition\\_pca.py:474\u001b[0m, in \u001b[0;36mPCA.fit_transform\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m    451\u001b[0m \u001b[38;5;129m@_fit_context\u001b[39m(prefer_skip_nested_validation\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m    452\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfit_transform\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, y\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m    453\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Fit the model with X and apply the dimensionality reduction on X.\u001b[39;00m\n\u001b[0;32m    454\u001b[0m \n\u001b[0;32m    455\u001b[0m \u001b[38;5;124;03m    Parameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    472\u001b[0m \u001b[38;5;124;03m    C-ordered array, use 'np.ascontiguousarray'.\u001b[39;00m\n\u001b[0;32m    473\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 474\u001b[0m     U, S, _, X, x_is_centered, xp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    475\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m U \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    476\u001b[0m         U \u001b[38;5;241m=\u001b[39m U[:, : \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_components_]\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\decomposition\\_pca.py:547\u001b[0m, in \u001b[0;36mPCA._fit\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    545\u001b[0m \u001b[38;5;66;03m# Call different fits for either full or truncated SVD\u001b[39;00m\n\u001b[0;32m    546\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fit_svd_solver \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfull\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcovariance_eigh\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m--> 547\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit_full\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_components\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mxp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_array_api_compliant\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    548\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fit_svd_solver \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marpack\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrandomized\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[0;32m    549\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fit_truncated(X, n_components, xp)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\decomposition\\_pca.py:561\u001b[0m, in \u001b[0;36mPCA._fit_full\u001b[1;34m(self, X, n_components, xp, is_array_api_compliant)\u001b[0m\n\u001b[0;32m    557\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    558\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mn_components=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmle\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m is only supported if n_samples >= n_features\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    559\u001b[0m         )\n\u001b[0;32m    560\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;241m0\u001b[39m \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m n_components \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmin\u001b[39m(n_samples, n_features):\n\u001b[1;32m--> 561\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    562\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mn_components=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mn_components\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must be between 0 and \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    563\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmin(n_samples, n_features)=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mmin\u001b[39m(n_samples,\u001b[38;5;250m \u001b[39mn_features)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m with \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    564\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msvd_solver=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fit_svd_solver\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    565\u001b[0m     )\n\u001b[0;32m    567\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmean_ \u001b[38;5;241m=\u001b[39m xp\u001b[38;5;241m.\u001b[39mmean(X, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m    568\u001b[0m \u001b[38;5;66;03m# When X is a scipy sparse matrix, self.mean_ is a numpy matrix, so we need\u001b[39;00m\n\u001b[0;32m    569\u001b[0m \u001b[38;5;66;03m# to transform it to a 1D array. Note that this is not the case when X\u001b[39;00m\n\u001b[0;32m    570\u001b[0m \u001b[38;5;66;03m# is a scipy sparse array.\u001b[39;00m\n\u001b[0;32m    571\u001b[0m \u001b[38;5;66;03m# TODO: remove the following two lines when scikit-learn only depends\u001b[39;00m\n\u001b[0;32m    572\u001b[0m \u001b[38;5;66;03m# on scipy versions that no longer support scipy.sparse matrices.\u001b[39;00m\n",
      "\u001b[1;31mValueError\u001b[0m: n_components=32 must be between 0 and min(n_samples, n_features)=1 with svd_solver='full'"
     ]
    }
   ],
   "source": [
    "hybrid_features = []\n",
    "for img in tqdm(images, desc=\"Extracting hybrid features\"):\n",
    "    feats = extract_hybrid_features(img)\n",
    "    hybrid_features.append(feats)\n",
    "\n",
    "hybrid_features = np.array(hybrid_features)\n",
    "print(\"Hybrid feature shape:\", hybrid_features.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab060c31-f7db-4adb-bba7-8d027d0297e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(hybrid_features)\n",
    "y = labels\n",
    "\n",
    "print(\"Feature vector normalized. Ready for QCNN input or classical classifier.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4d4876c-d455-4b53-81d1-8363dd20c4b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_file = \"hybrid_encoded_petimages_qcnn.npz\"\n",
    "np.savez_compressed(output_file, X=X, y=y)\n",
    "\n",
    "print(f\"Hybrid encoded features saved to: {output_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6807e8a7-8458-4961-852f-a72efc799f5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "data = np.load(\"hybrid_encoded_petimages_qcnn.npz\")\n",
    "X_loaded = data['X']\n",
    "y_loaded = data['y']\n",
    "\n",
    "# Print first 5 examples\n",
    "for i in range(5):\n",
    "    print(f\"Label: {y_loaded[i]}, Encoded Feature: {X_loaded[i]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "962f3319-ad8e-4676-9fb6-abb941a222c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 2000 images with shape (64, 64)\n",
      "Fitting PCA model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting hybrid features: 100%|█████████████████████████████████████████████████| 2000/2000 [00:02<00:00, 720.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hybrid feature shape: (2000, 2336)\n",
      "Feature vector normalized. Ready for QCNN input or classical classifier.\n",
      "Hybrid encoded features saved to: hybrid_encoded_petimages_qcnn.npz\n",
      "Label: 0, Encoded Feature Sample: [2.21976709 3.88969971 1.6704381  3.00981385 3.44670793 3.26147403\n",
      " 2.99879303 3.70665639 3.51437865 2.77287502]\n",
      "Label: 0, Encoded Feature Sample: [-0.36989287 -0.39360712 -0.39191396 -0.40107958 -0.39085865 -0.37762145\n",
      " -0.39222142 -0.39390295 -0.38721022 -0.38012319]\n",
      "Label: 0, Encoded Feature Sample: [-0.36989287 -0.39360712 -0.39191396 -0.40107958 -0.39085865 -0.37762145\n",
      " -0.39222142 -0.39390295 -0.38721022 -0.38012319]\n",
      "Label: 0, Encoded Feature Sample: [-0.36989287 -0.39360712 -0.39191396 -0.40107958 -0.39085865 -0.37762145\n",
      " -0.39222142 -0.39390295 -0.38721022 -0.38012319]\n",
      "Label: 0, Encoded Feature Sample: [-0.36989287 -0.39360712 -0.39191396 -0.40107958 -0.39085865 -0.37762145\n",
      " -0.39222142 -0.39390295 -0.38721022 -0.38012319]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# --- Load Dataset ---\n",
    "dataset_path = \"D:/datasets/PetImages\"\n",
    "categories = [\"Cat\", \"Dog\"]\n",
    "num_images_per_category = 1000\n",
    "images = []\n",
    "labels = []\n",
    "\n",
    "for label, category in enumerate(categories):\n",
    "    category_path = os.path.join(dataset_path, category)\n",
    "    count = 0\n",
    "    for filename in os.listdir(category_path):\n",
    "        if count >= num_images_per_category:\n",
    "            break\n",
    "        try:\n",
    "            img_path = os.path.join(category_path, filename)\n",
    "            img = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE)\n",
    "            if img is None:\n",
    "                continue\n",
    "            img = cv2.resize(img, (64, 64))\n",
    "            images.append(img)\n",
    "            labels.append(label)\n",
    "            count += 1\n",
    "        except Exception as e:\n",
    "            continue\n",
    "\n",
    "images = np.array(images)\n",
    "labels = np.array(labels)\n",
    "print(f\"Loaded {len(images)} images with shape {images[0].shape}\")\n",
    "\n",
    "# --- Hybrid Feature Extraction ---\n",
    "def extract_hybrid_features(img, pca_model=None, pca_components=32):\n",
    "    orb = cv2.ORB_create(nfeatures=64)\n",
    "    keypoints, descriptors = orb.detectAndCompute(img, None)\n",
    "    if descriptors is None:\n",
    "        descriptors = np.zeros((64, 32), dtype=np.uint8)\n",
    "    elif descriptors.shape[0] < 64:\n",
    "        descriptors = np.vstack([descriptors, np.zeros((64 - descriptors.shape[0], 32), dtype=np.uint8)])\n",
    "    else:\n",
    "        descriptors = descriptors[:64]\n",
    "    orb_features = descriptors.flatten()\n",
    "\n",
    "    img_norm = img / 255.0\n",
    "    flat = img_norm.flatten()\n",
    "    if len(flat) < 256:\n",
    "        padded = np.pad(flat, (0, 256 - len(flat)))\n",
    "    else:\n",
    "        padded = flat[:256]\n",
    "    amp_features = padded\n",
    "\n",
    "    flattened_img = img.flatten().reshape(1, -1)\n",
    "    if pca_model is not None:\n",
    "        pca_data = pca_model.transform(flattened_img).flatten()\n",
    "    else:\n",
    "        pca_data = np.zeros(pca_components)\n",
    "\n",
    "    hybrid_feature_vector = np.concatenate((orb_features, amp_features, pca_data))\n",
    "    return hybrid_feature_vector\n",
    "\n",
    "# --- PCA Training ---\n",
    "print(\"Fitting PCA model...\")\n",
    "flat_images = images.reshape(images.shape[0], -1)\n",
    "pca_model = PCA(n_components=32)\n",
    "pca_model.fit(flat_images)\n",
    "\n",
    "# --- Feature Vector Construction ---\n",
    "hybrid_features = []\n",
    "for img in tqdm(images, desc=\"Extracting hybrid features\"):\n",
    "    feats = extract_hybrid_features(img, pca_model=pca_model, pca_components=32)\n",
    "    hybrid_features.append(feats)\n",
    "\n",
    "hybrid_features = np.array(hybrid_features)\n",
    "print(\"Hybrid feature shape:\", hybrid_features.shape)\n",
    "\n",
    "# --- Standardize Features ---\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(hybrid_features)\n",
    "y = labels\n",
    "print(\"Feature vector normalized. Ready for QCNN input or classical classifier.\")\n",
    "\n",
    "# --- Save Features ---\n",
    "output_file = \"hybrid_encoded_petimages_qcnn.npz\"\n",
    "np.savez_compressed(output_file, X=X, y=y)\n",
    "print(f\"Hybrid encoded features saved to: {output_file}\")\n",
    "\n",
    "# --- Load & Verify ---\n",
    "data = np.load(output_file)\n",
    "X_loaded = data['X']\n",
    "y_loaded = data['y']\n",
    "for i in range(5):\n",
    "    print(f\"Label: {y_loaded[i]}, Encoded Feature Sample: {X_loaded[i][:10]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3377030e-408f-469a-b418-be31659caf00",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pennylane",
   "language": "python",
   "name": "pennylane"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
