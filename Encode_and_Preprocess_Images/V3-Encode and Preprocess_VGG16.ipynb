{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6491cbc-ab70-4823-bdf1-c30053042553",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms as transforms\n",
    "import pennylane as qml\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Define dataset path\n",
    "dataset_path = \"D:/datasets/PetImages\"\n",
    "categories = [\"Cat\", \"Dog\"]\n",
    "\n",
    "# Check if dataset exists\n",
    "if not os.path.exists(dataset_path):\n",
    "    raise FileNotFoundError(\"Dataset directory not found!\")\n",
    "\n",
    "# Define Image Transformations\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToPILImage(),\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "# Load Pretrained VGG16 Model\n",
    "vgg16 = models.vgg16(pretrained=True)\n",
    "model = nn.Sequential(*list(vgg16.children())[:-1])  # Remove final classifier\n",
    "model.eval()\n",
    "\n",
    "def load_images(category):\n",
    "    category_path = os.path.join(dataset_path, category)\n",
    "    if not os.path.exists(category_path):\n",
    "        raise FileNotFoundError(f\"{category} folder not found!\")\n",
    "    \n",
    "    images = []\n",
    "    for img_name in os.listdir(category_path)[:100]:  # Load limited images for testing\n",
    "        img_path = os.path.join(category_path, img_name)\n",
    "        img = cv2.imread(img_path)\n",
    "        if img is not None:\n",
    "            img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "            img = transform(img)\n",
    "            images.append(img)\n",
    "    \n",
    "    return torch.stack(images) if images else torch.empty(0)\n",
    "\n",
    "def extract_features(images):\n",
    "    if images.shape[0] == 0:\n",
    "        return np.array([])\n",
    "    with torch.no_grad():\n",
    "        features = model(images).squeeze()\n",
    "    return features.view(features.shape[0], -1).numpy()\n",
    "\n",
    "def amplitude_encoding(features):\n",
    "    feature_len = len(features[0])  # 25088 for VGG16\n",
    "    num_qubits = int(np.ceil(np.log2(feature_len)))  # Next power of 2 (15 qubits for 32768)\n",
    "\n",
    "    # Pad feature vector to match 2^num_qubits\n",
    "    padded_len = 2 ** num_qubits\n",
    "    padded_features = np.zeros((len(features), padded_len))\n",
    "    padded_features[:, :feature_len] = features  # Fill original values\n",
    "\n",
    "    dev = qml.device(\"default.qubit\", wires=num_qubits)\n",
    "\n",
    "    @qml.qnode(dev)\n",
    "    def encode_feature_vector(feature_vector):\n",
    "        qml.templates.AmplitudeEmbedding(feature_vector, wires=range(num_qubits), normalize=True)\n",
    "        return [qml.expval(qml.PauliZ(i)) for i in range(num_qubits)]\n",
    "\n",
    "    return np.array([encode_feature_vector(f) for f in padded_features])\n",
    "\n",
    "\n",
    "# Process Data\n",
    "encoded_data = {}\n",
    "for category in categories:\n",
    "    print(f\"Processing {category} images...\")\n",
    "    images = load_images(category)\n",
    "    features = extract_features(images)\n",
    "    qfeatures = amplitude_encoding(features)\n",
    "    encoded_data[category] = qfeatures\n",
    "\n",
    "# Save Encoded Data\n",
    "with open(\"quantum_encoded_dataf-VGG16.pkl\", \"wb\") as f:\n",
    "    pickle.dump(encoded_data, f)\n",
    "\n",
    "print(\"Quantum encoded data saved successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c29b636a-8528-4e04-a79a-fea413c3d946",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Key: Cat\n",
      "  Shape: (100, 15)\n",
      "  Sample Data: [[ 0.30744584  0.3140313  -0.01021218  0.00650647 -0.01859961  0.00208803\n",
      "   0.03067205 -0.02548145  0.04621609  0.01535632 -0.01638981  0.04043715\n",
      "   0.00401886  0.0125727   0.00378175]\n",
      " [ 0.38016246  0.15620541 -0.10914964 -0.21418308 -0.11442594  0.0019177\n",
      "   0.09973423 -0.09932011 -0.17917055 -0.0273188  -0.02485525  0.02962766\n",
      "   0.06984065 -0.01333068  0.00709992]\n",
      " [ 0.1827091   0.22371495 -0.09715515  0.03010768  0.01085691  0.04966486\n",
      "   0.11637957  0.06302308 -0.14814628  0.02007559  0.0053967   0.00870771\n",
      "   0.09983913 -0.02458192  0.00096232]\n",
      " [ 0.23736537  0.29812186 -0.02755987 -0.09567766  0.02821702  0.08294426\n",
      "   0.0229663   0.06099487 -0.02896065  0.10892302 -0.15556594  0.03920613\n",
      "   0.04725985  0.01876614 -0.00138254]\n",
      " [ 0.23591991  0.28449232 -0.10468675 -0.02277449  0.0235628   0.09927221\n",
      "  -0.00257819  0.05346106 -0.03587455  0.0171419   0.06819305  0.00411254\n",
      "   0.07810726  0.00429773  0.012983  ]]\n",
      "\n",
      "==================================================\n",
      "\n",
      "Key: Dog\n",
      "  Shape: (100, 15)\n",
      "  Sample Data: [[ 1.78473582e-01  3.58256065e-01  1.54149131e-01  1.27399971e-01\n",
      "  -4.42656298e-02 -5.34657836e-02  4.80818418e-03  1.09795752e-01\n",
      "  -2.90508317e-02 -1.57531052e-01 -4.12573813e-02 -2.10987474e-02\n",
      "   2.72376524e-02  3.93995015e-02  2.14862327e-03]\n",
      " [ 3.77022310e-01  1.83302974e-01  7.54152372e-02  7.25011759e-02\n",
      "  -3.84097210e-03  1.76230838e-02  9.04693838e-02 -3.07004767e-02\n",
      "  -2.33659366e-04 -1.68227580e-02  2.84369345e-02 -5.44908609e-03\n",
      "   1.08037401e-01 -8.40599084e-04 -4.92035943e-03]\n",
      " [ 5.51776889e-01  1.70693279e-01 -1.05305772e-01  2.28474356e-01\n",
      "  -1.95658594e-01  9.19355389e-02 -8.47854398e-02 -1.21995008e-01\n",
      "   4.00428087e-02  1.14003609e-01  3.46831217e-03  1.99760541e-02\n",
      "   7.91044954e-02 -7.17631312e-02  2.64056410e-03]\n",
      " [ 2.81099333e-01  2.90712110e-01 -4.44288092e-02  6.03599885e-02\n",
      "   7.60850431e-03 -1.80116224e-03  1.27104464e-02  3.39123987e-02\n",
      "   1.65152994e-02 -4.81829906e-02  2.57201198e-04  1.11075514e-02\n",
      "  -3.87376683e-03  3.01554116e-02 -5.18045328e-03]\n",
      " [ 2.99766105e-01  2.65455037e-01  1.43807102e-01  1.02671944e-01\n",
      "  -5.32780254e-02  2.38302773e-02 -7.79069868e-02 -6.03431055e-02\n",
      "   1.47440924e-02  2.99117825e-02  8.25882674e-05  1.64760811e-02\n",
      "  -1.09083979e-02 -1.34404734e-02  6.68519865e-03]]\n",
      "\n",
      "==================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "with open(\"quantum_encoded_dataf-VGG16.pkl\", \"rb\") as f:\n",
    "    data = pickle.load(f)\n",
    "\n",
    "# Display contents head\n",
    "for key, value in data.items():\n",
    "    print(f\"Key: {key}\")\n",
    "\n",
    "    if hasattr(value, 'shape'):\n",
    "        print(f\"  Shape: {value.shape}\")\n",
    "        print(f\"  Sample Data: {value[:5]}\")  \n",
    "\n",
    "    else:\n",
    "        print(f\"  Value: {value}\")\n",
    "\n",
    "    print(\"\\n\" + \"=\"*50 + \"\\n\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pennylane",
   "language": "python",
   "name": "pennylane"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
